Ex3.1

Dating with girls: How to win her hearts by a series of acitvities.  
states: My and her free time, My pocket money, relationship with the girl: unknown, acquainted, familiar, close or Girlfriend  
rewards: kiss +10,  hand in hand +5, smile +1, nice guy -1, smack -5  
actions: study together, chat, take a lunch, watch a movie, go to an amusement park, travel, send a gift  
However, I hope so.

Ex3.2

Ex3.3

I think the line between the agent and environment depends on the specific circumstance. When we are learning how to drive, we should focus on the accelerators, brake and so on. Neither want we want to go or the torque from the rubber counts. For a car racer, the tire torque really matter since it counld influence the speed of the car greatly. And for veteran drivers, they konw their car very well and are unnecessary to worry about the detailed acceleration and brake since driving has been something like instant for them. So, say if they want to save the petrol, they should decide the route for the day.  
I think these locations vary in trems of the abstract level. The brain decision is more abstract than the body meeting the machine since there are many other ways to reach the location like the public transit. And there are many different combinations of acceleration and brake to exert the same tire torque on the road. These locations should match the abstract level we concern I think.

Ex3.4

The ruturn wuold be ![equation](http://www.sciweavers.org/tex2img.php?eq=%5Cgamma%5Ek&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0) , k is the number of time steps before failure. But the continuing formulation will contain other episodes and the k on the exponent in future episodes would take the time step in the former episodes into account. I am confused. Since the rewards would be given after each episode. Why there is a reward associated with each time step in the episodic view?   

Ex3.5

I don't think I have given the effective info to the agent. Maximize the return doesn't necessarily mean the ability to pass the maze. Say if the maze is simple enough, it will always find a way out by random trials. So the agent gets the reward every time and doesn't need to make any change. It would keep trying randomly and thus show no evidence of learning.

Ex3.6

After seeing the first scene, I, the vision system, don't catch the MArkov state of the environment. Say if the sun lies behind me and I couldn't see it, I'm unable to predict the changes of the light, caused by the rise of the Sun, on the objects I see. Otherwise, if the state is Markov, it's a pretty nice thing since we could just catch a sight of the universe and know all about it. On the other hand, the complete darkness from the broken camera is a Markov state. I know I'm going to spend the day in the endless and frightening darkness. 
